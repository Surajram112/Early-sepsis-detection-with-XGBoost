{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import os\n",
    "import base64\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pandas_gbq\n",
    "import random\n",
    "from tqdm.notebook import tnrange\n",
    "from ipywidgets import IntProgress\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "# Print current working directory\n",
    "print(\"Current working dir : %s\" % os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_filename = 'ICU_SEPSIS3'\n",
    "query = \"\"\"SELECT * FROM `vibrant-shell-313523.MIMIC_IV_v1.ICU_SEPSIS3_MOD`\"\"\"\n",
    "hours_prev = 24\n",
    "regen = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### ICU Reduced Filtered dataset #######################################################\n",
    "if not os.path.isfile(os.path.join(os.getcwd(),(base_filename +'_ALL.pickle'))) or regen:\n",
    "    df = pandas_gbq.read_gbq(query, project_id=\"vibrant-shell-313523\", use_bqstorage_api=True, progress_bar_type='tqdm')\n",
    "    \n",
    "    with open(os.path.join(os.getcwd(),(base_filename +'_ALL.pickle')), 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "else:\n",
    "    with open(os.path.join(os.getcwd(),(base_filename +'_ALL.pickle')), 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "print(f'Dataset loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with 100% missing values\n",
    "df = df[df.columns[df.isnull().mean() != 1.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Sample of dataset ####################\n",
    "df.sort_values(by=['stay_id','hr'], axis=0, ascending=True, inplace=True, ignore_index=True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stay = df.stay_id.unique()\n",
    "dictOfIDs = { i : np.where(df_stay == i)[0][0] for i in df_stay}\n",
    "\n",
    "# Remap the values of the dataframe\n",
    "df[\"PatientID\"] = df[\"stay_id\"].map(dictOfIDs)\n",
    "\n",
    "###### Sample of dataset #######\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcfa8b8",
   "metadata": {},
   "source": [
    "## Shifting labels 6 or 12 hours ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fc85ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hours_prev != 0:\n",
    "    label_shifted = pd.DataFrame(df.groupby('PatientID')['SepsisLabel'].shift(-hours_prev).ffill().astype(int), columns=['SepsisLabel'])\n",
    "    df_dropped_labels = df.drop(columns=['SepsisLabel'])\n",
    "    df = pd.concat([df_dropped_labels,label_shifted],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Filling Weight, Age, Ethnicity and Gender (Administrative Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_column_forward_fill(df,columns):\n",
    "    df_seg = df[columns].copy(deep=True)\n",
    "    columns.remove('PatientID')\n",
    "    columns.remove('hr')\n",
    "    df_dropped = df.drop(columns=columns)\n",
    "    df_ff = df_seg.groupby('stay_id').ffill()\n",
    "    df_ff_merged = pd.merge(df_dropped,df_ff, how='left', left_on=['PatientID','hr'], right_on = ['PatientID','hr'])\n",
    "    return df_ff_merged.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_feats = list(['stay_id','weight','admission_age','gender','PatientID','hr'])\n",
    "df_f = group_column_forward_fill(df, admin_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Data\n",
    "object_list = []\n",
    "for column in df_f.columns:\n",
    "    if df_f[column].dtype == object or df_f[column].dtype == bool:\n",
    "        # print(f\"{column} with unique type {df[column].unique()}\")\n",
    "        object_list.append(column)\n",
    "df_cat = df_f[object_list + ['PatientID','hr']]\n",
    "df_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the variable\n",
    "df_cat_enc = pd.get_dummies(df_cat, columns=object_list, dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef379500",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting only values in clinical ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Data, Excludind Label and Patient ID\n",
    "df_num = df_f[df_f.columns.difference(object_list)].copy(deep=True)\n",
    "df_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_df = pd.read_excel(os.path.join(os.getcwd(),'clinical_ranges.xlsx')).set_index('Column Name')\n",
    "display(range_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, range_df,percent_change=0.75):\n",
    "    df_new = df.copy(deep=True)\n",
    "    for column in df[df.columns.difference(['SepsisLabel', 'PatientID',  'hr', 'stay_id','SOFA_24h','los_hospital'])].columns:\n",
    "        # Calculate true limits\n",
    "        try:\n",
    "            max = pd.to_numeric(range_df.loc[column,'Maximum'].max())\n",
    "            min = pd.to_numeric(range_df.loc[column,'Minimum'].min())\n",
    "        except:\n",
    "            max = pd.to_numeric(range_df.loc[column,'Maximum'])\n",
    "            min = pd.to_numeric(range_df.loc[column,'Minimum'])\n",
    "        deviation = (max-min)*percent_change\n",
    "        if min == 0: \n",
    "            low_limit = 0\n",
    "        else:\n",
    "            low_limit = min - deviation\n",
    "        high_limit = max + deviation\n",
    "        # Apply filter with respect to IQR, including optional whiskers\n",
    "        filter = (df[column] >= low_limit) & (df[column] <= high_limit)\n",
    "        percent_retained = (df[column].between(low_limit,high_limit).sum()/df[column].count())*100\n",
    "        print('Keep',percent_retained,'%\\ of',column)\n",
    "        df_new[column] = df[column].loc[filter]\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_mod = remove_outliers(df_num, range_df, percent_change=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_mod.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_mod.isna().sum().sum() - df_num.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Data Normalization #########################################\n",
    "scaler = MinMaxScaler((0,1))\n",
    "\n",
    "# Copy dataframe to just substitute later\n",
    "df_num_scaled = df_num_mod.copy(deep=True)\n",
    "\n",
    "# Extract only the columns to be scaled  \n",
    "df_num_s = df_num_mod[df_num_mod.columns.difference(['SepsisLabel', 'PatientID',  'hr', 'stay_id','SOFA_24h'])]\n",
    "\n",
    "# Scale the data\n",
    "df_num_scaled[df_num_mod.columns.difference(['SepsisLabel', 'PatientID',  'hr', 'stay_id','SOFA_24h'])] = scaler.fit_transform(df_num_s)\n",
    "\n",
    "# Save scaler values to import after testing\n",
    "if hours_prev != 0:\n",
    "    scaler_filename = \"scaler_onset_pre_\" + str(hours_prev) + \"hr.pickle\"\n",
    "else:\n",
    "    scaler_filename = \"scaler_onset.pickle\"\n",
    "\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "Feat_Min_Max = pd.DataFrame({'Feature':df_num_s.columns, 'Min':scaler.data_min_, 'Max':scaler.data_max_})\n",
    "Feat_Min_Max.to_csv(os.path.join(os.getcwd(),'Feature_Min_Max.csv'))\n",
    "\n",
    "# Display Min and Max\n",
    "display(Feat_Min_Max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join and save Encoded, Scaled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc_scaled = pd.merge(df_num_scaled, df_cat_enc, how='left', left_on=['PatientID','hr'], right_on = ['PatientID','hr'])\n",
    "df_enc_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc502ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hours_prev !=0:\n",
    "    ################################### ICU Onset Pre-Processed Dataset #######################################################\n",
    "    with open(os.path.join(os.getcwd(),(base_filename+'_ENC_SCALED_PRE_'+ str(hours_prev) +'HR.pickle')), 'wb') as f:\n",
    "        pickle.dump(df_enc_scaled, f)\n",
    "else:\n",
    "    ################################### ICU Onset Pre-Processed Dataset #######################################################\n",
    "    with open(os.path.join(os.getcwd(),(base_filename +'_ENC_SCALED.pickle')), 'wb') as f:\n",
    "        pickle.dump(df_enc_scaled, f)\n",
    "\n",
    "print(f'Dataset Saved with labels ',hours_prev,' hours pervious to diagnosis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Imputation Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolating values per patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_scaled_inter = df_num_scaled.groupby('PatientID').apply(lambda group: group.interpolate(limit_area='inside')).reset_index(drop=True)\n",
    "df_num_scaled_inter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join and Save Encoded, Scaled and Interpolated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc_scaled_inter = pd.merge(df_num_scaled_inter, df_cat_enc, how='left', left_on=['PatientID','hr'], right_on = ['PatientID','hr'])\n",
    "df_enc_scaled_inter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904bc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hours_prev !=0:\n",
    "    ################################### ICU Onset Pre-Processed Dataset #######################################################\n",
    "    with open(os.path.join(os.getcwd(),(base_filename+'_ENC_SCALED_INTER_'+ str(hours_prev) +'HR.pickle')), 'wb') as f:\n",
    "        pickle.dump(df_num_scaled_inter, f)\n",
    "else:\n",
    "    ################################### ICU Onset Pre-Processed Dataset #######################################################\n",
    "    with open(os.path.join(os.getcwd(),(base_filename +'_ENC_SCALED_INTER.pickle')), 'wb') as f:\n",
    "        pickle.dump(df_num_scaled_inter, f)\n",
    "\n",
    "print(f'Dataset Saved with labels ',hours_prev,' hours pervious to diagnosis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f148df98",
   "metadata": {},
   "source": [
    "# Step 3: Imputation Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratification by Admission SOFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_enc_scaled_inter.groupby(['hr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_SOFA = df_grouped.get_group(1)[['PatientID','SOFA_24h']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_SOFA_group = init_SOFA.groupby('SOFA_24h')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634a8ed",
   "metadata": {},
   "source": [
    "## Fill Forwards/median imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "df_pat = pd.DataFrame(columns=df_enc_scaled_inter.columns)\n",
    "for key,item in init_SOFA_group:\n",
    "  patients_SOFA = init_SOFA_group.get_group(key)['PatientID'].values\n",
    "  patients_SOFA_data = df_enc_scaled_inter.loc[df_enc_scaled_inter['PatientID'].isin(patients_SOFA)]\n",
    "  print('Missing before imputation:',patients_SOFA_data.isna().sum().sum())\n",
    "  \n",
    "  cols = []\n",
    "  for col in patients_SOFA_data:\n",
    "    if patients_SOFA_data[col].std() > 0.1 and patients_SOFA_data[col].std() <= 1:\n",
    "      cols.append(col)\n",
    "\n",
    "  patients_SOFA_ffill = patients_SOFA_data.groupby('PatientID').ffill()[cols]\n",
    "  patients_SOFA_median = patients_SOFA_data[patients_SOFA_data.columns.difference(cols)].fillna(patients_SOFA_data[patients_SOFA_data.columns.difference(cols)].median())#.reset_index(drop=True)\n",
    "  patients_SOFA_data_filled = pd.concat([patients_SOFA_ffill,patients_SOFA_median],axis=1)\n",
    "\n",
    "  print('Missing after imputation:',patients_SOFA_data_filled.isna().sum().sum())\n",
    "  df_list.append(patients_SOFA_data_filled)\n",
    "\n",
    "full_df = pd.concat(df_list).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427b9514",
   "metadata": {},
   "source": [
    "# Save Encoded, Scaled, Interpolated and Foward filled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7058070",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hours_prev !=0:\n",
    "    ################################### ICU Onset Pre-Processed Dataset #######################################################\n",
    "    with open(os.path.join(os.getcwd(),(base_filename+'_ENC_SCALED_INTER_FFILLorMEDIAN_'+ str(hours_prev) +'HR.pickle')), 'wb') as f:\n",
    "        pickle.dump(full_df, f)\n",
    "else:\n",
    "    ################################### ICU Onset Pre-Processed Dataset #######################################################\n",
    "    with open(os.path.join(os.getcwd(),(base_filename +'_ENC_SCALED_INTER_FFILLorMEDIAN.pickle')), 'wb') as f:\n",
    "        pickle.dump(full_df, f)\n",
    "\n",
    "print(f'Dataset Saved with labels ',hours_prev,' hours pervious to diagnosis')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5432f8d548eaaebb87d8c4e2cf1b237e1b330c3418db9f0fb5a8137eaa7957e9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('rl_agents': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "5432f8d548eaaebb87d8c4e2cf1b237e1b330c3418db9f0fb5a8137eaa7957e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
